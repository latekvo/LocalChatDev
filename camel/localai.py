# localai | This file is a recreation of the `openai` library, with the backend calling ollama models running locally
# for now, it calls models that work on the local machine,
# and soon we will implement a system of workers onto whom requests will be offloaded.
# the reason this library is so convoluted, replicating every unnecessary aspect of OpenAI's library
# is because i want to eventually get this file working as a standalone library, as well as keeping my development debt
# as low as possible, and avoiding quick hacks has been reliably the best method for me for avoiding unnecessary bugs.

from typing import List, Optional
from typing_extensions import Literal

import json
import math
import time

# Update: new openai api natively supports choosing ai-server, thus i will be switching to recreating that
# Additionally, after some planning it seems like both approaches may be equally difficult to implement,
# and so I will try writing the approach #2 straight away.

import requests
from types import SimpleNamespace
from dataclasses import dataclass


# todo: write a worker-app
# todo: add a system automatically choosing the most appropriate model, handle this in the worker-app
# due to how 'client' is initialized in other files, distribution will be handled here via a separate singleton class
# todo: append RUN_LOCALLY logic to every instance of openai and OPENAI_API_KEY inside the CAMEL folder (recursive)

# all of the following classes: LocalChatCompletionMessage, LocalCompletionUsage, LocalChoice, LocalChatCompletion
# are used solely for the purpose of type recognition in chat_agent.py
@dataclass
class LocalChatCompletionMessage:
    content: Optional[str]  # The contents of the message.
    role: Literal["assistant"]  # The role of the author of this message.

    # function_call: Optional[FunctionCall] = None
    """Deprecated and replaced by `tool_calls`.

    The name and arguments of a function that should be called, as generated by the
    model.
    """

    # fixme: this is a very important feature
    # tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None


@dataclass
class LocalCompletionUsage:
    prompt_tokens: str
    completion_tokens: str
    total_tokens: str


@dataclass
class LocalChoice:
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `content_filter` if content was omitted due to a flag from our content
    filters, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""

    message: LocalChatCompletionMessage
    """A chat completion message generated by the model."""


@dataclass
class LocalChatCompletion:
    id: str  # A unique identifier for the chat completion.

    choices: List[LocalChoice]  # A list of chat completion choices.

    created: int  # The Unix timestamp (in seconds) of when the chat completion was created.

    model: str  # The model used for the chat completion.

    object: Literal["chat.completion"]  # The object type, which is always `chat.completion`.

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with.

    Can be used in conjunction with the `seed` request parameter to understand when
    backend changes have been made that might impact determinism.
    """

    usage: Optional[LocalCompletionUsage] = None  # Usage statistics for the completion request.


# a basic singleton implementation, communicates with all the worker agents
class WorkerManagerMetaclass:
    _instances = {}

    def __call__(self, *args, **kwargs):
        if self not in self._instances:
            self._instances[self] = super(WorkerManagerMetaclass, self).__call__(*args, **kwargs)
        return self._instances[self]


# won't be used for now, first we have to get rest of the functionality working.
class WorkerManager(WorkerManagerMetaclass):
    def __init__(self, data):
        self.data = data


class LocalAI:
    class Chat:
        class Completions:
            # todo: replace with all parameters that are mentioned in either web_spider.py or model_backend.py
            def create(self, user, messages, max_tokens, *args, **kwargs):
                # all supplied kwargs: ['messages', 'model', 'temperature', 'top_p', 'n', 'stream', 'stop',
                # 'max_tokens', 'presence_penalty', 'frequency_penalty', 'logit_bias', 'user']

                request_url = self.parent.parent.base_url + 'api/generate'
                # this broken formatting wraps the json inside the key of our html form,
                # this is the only accepted formatting by ollama
                # request_headers = {"Content-Type": "application/x-www-form-urlencoded"}
                request_data = {
                    'model': self.parent.parent.model,
                    'prompt': 'hello',
                    'system': 'talk like a pirate',
                }

                response = requests.post(url=request_url, json=request_data, stream=True)
                response.raise_for_status()

                response_stream = response.iter_lines()
                response_list = []

                # a frequent bug with llama-uncensored2 is to have a soft-locked loop of the '\n' token being returned
                # in order to avoid this we have to switch to a stream mode, parsing every incoming token individually
                repeat_counter = 0
                repeat_token = ''

                # convert response to json:
                for chunk in response_stream:
                    chunk_text = json.loads(chunk)['response']
                    chunk_done = json.loads(chunk)['done']

                    if chunk_text == repeat_token:
                        repeat_counter += 1
                    else:
                        repeat_counter = 0
                        repeat_token = chunk_text

                    # print('partial:', chunk_text)
                    # we break early to avoid breaking the response_text string
                    if chunk_done or repeat_counter > 5:
                        response.close()
                        break
                    else:
                        response_list.append(chunk_text)

                response_text = ''.join(response_list)

                print('done')
                print('^ entirety:', response_text)

                # token estimations, todo: make use of the llama tokenizer
                input_cost = 0
                output_cost = math.ceil(len(response_list) / 3)
                total_cost = input_cost + output_cost

                # replicate the entire returned object: https://platform.openai.com/docs/api-reference/chat/object
                # todo: i may need to convert this simple namespace into a ChatCompletion class
                # SimpleNamespace just creates an object in place, it's like having a no-name class
                return_object = LocalChatCompletion(
                    id=str(round(time.time() * 1000)),  # fixme: timestamp is not really a proper id, works for now
                    object='chat.completion',
                    created=round(time.time() * 1000),
                    model=self.parent.parent.model,
                    system_fingerprint='system_fingerprint-stud',
                    choices=[
                        LocalChoice(
                            # this section requires some testing to be completed
                            # so far it seems to be sufficient to only have one response here,
                            # but having more alternative choices should work as well
                            index=0,
                            message=LocalChatCompletionMessage(
                                role='assistant',
                                content=response_text
                            ),
                            finish_reason='stop'
                        )
                    ],
                    usage=LocalCompletionUsage(
                        # this section requires some testing to be completed
                        prompt_tokens=str(input_cost),
                        completion_tokens=str(output_cost),  # a very rough, pessimistic estimate
                        total_tokens=str(total_cost)
                    )
                )

                print('returning')
                print('^ object:', return_object)

                return return_object

            def __init__(self, parent):
                self.parent = parent

        def __init__(self, parent):
            self.parent = parent

            # Create instances of all nested classes
            self.completions = self.Completions(self)

    def __init__(self, base_url=None, decentralize=False):
        # base_url will only ever be used when DECENTRALIZE is set to 0 or not set at all
        if base_url:
            self.base_url = base_url
        else:
            self.base_url = 'http://localhost:11434/'

        self.model = 'llama2-uncensored:7b'  # todo: move model selection to the worker-app

        # Create instances of all nested classes
        self.chat = self.Chat(self)
